{
  "filename": "table_images.pdf",
  "total_pages": 9,
  "pages": [
    {
      "page_number": 1,
      "texts": [
        {
          "text": "4\n2\n0\n2\nc\ne\nD\n9",
          "bbox": {
            "x0": 16.34,
            "y0": 483.64,
            "x1": 36.34,
            "y1": 575.84
          }
        },
        {
          "text": "]\nL\nC\n.\ns\nc\n[",
          "bbox": {
            "x0": 16.34,
            "y0": 413.1,
            "x1": 36.34,
            "y1": 473.64
          }
        },
        {
          "text": "5\nv\n9\n6\n8\n9\n0\n.\n8\n0\n4\n2\n:\nv\ni\nX\nr\na",
          "bbox": {
            "x0": 16.34,
            "y0": 237.0,
            "x1": 36.34,
            "y1": 403.1
          }
        },
        {
          "text": "Docling Technical Report",
          "bbox": {
            "x0": 212.59,
            "y0": 551.16,
            "x1": 399.41,
            "y1": 568.38
          }
        },
        {
          "text": "Version 1.0",
          "bbox": {
            "x0": 283.31,
            "y0": 503.34,
            "x1": 328.69,
            "y1": 513.3
          }
        },
        {
          "text": "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos\nPanos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer\nKasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima\nValery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar",
          "bbox": {
            "x0": 113.64,
            "y0": 439.85,
            "x1": 498.36,
            "y1": 482.54
          }
        },
        {
          "text": "AI4K Group, IBM Research\nR¨uschlikon, Switzerland",
          "bbox": {
            "x0": 249.28,
            "y0": 408.0,
            "x1": 362.72,
            "y1": 428.87
          }
        },
        {
          "text": "Abstract",
          "bbox": {
            "x0": 283.76,
            "y0": 382.41,
            "x1": 328.24,
            "y1": 394.37
          }
        },
        {
          "text": "This technical report introduces Docling, an easy to use, self-contained, MIT-\nlicensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and table\nstructure recognition (TableFormer), and runs efficiently on commodity hardware\nin a small resource budget. The code interface allows for easy extensibility and\naddition of new features and models.",
          "bbox": {
            "x0": 143.86,
            "y0": 300.65,
            "x1": 468.14,
            "y1": 365.39
          }
        },
        {
          "text": "1",
          "bbox": {
            "x0": 108.0,
            "y0": 257.05,
            "x1": 113.98,
            "y1": 269.01
          }
        },
        {
          "text": "Introduction",
          "bbox": {
            "x0": 125.93,
            "y0": 257.05,
            "x1": 190.81,
            "y1": 269.01
          }
        },
        {
          "text": "Converting PDF documents back into a machine-processable format has been a major challenge\nfor decades due to their huge variability in formats, weak standardization and printing-optimized\ncharacteristic, which discards most structural features and metadata. With the advent of LLMs\nand popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich\ncontent embedded in PDFs has become ever more relevant. In the past decade, several powerful\ndocument understanding solutions have emerged on the market, most of which are commercial soft-\nware, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only\na handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap\nto proprietary solutions.",
          "bbox": {
            "x0": 108.0,
            "y0": 143.46,
            "x1": 504.0,
            "y1": 240.69
          }
        },
        {
          "text": "With Docling, we open-source a very capable and efficient document conversion tool which builds\non the powerful, specialized AI models and datasets for layout analysis and table structure recog-\nnition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple,\nself-contained python library with permissive license, running entirely locally on commodity hard-\nware. Its code architecture allows for easy extensibility and addition of new features and models.",
          "bbox": {
            "x0": 108.0,
            "y0": 83.44,
            "x1": 504.0,
            "y1": 137.26
          }
        },
        {
          "text": "Docling Technical Report",
          "bbox": {
            "x0": 108.0,
            "y0": 50.19,
            "x1": 200.51,
            "y1": 59.15
          }
        },
        {
          "text": "1",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 2,
      "texts": [
        {
          "text": "Here is what Docling delivers today:",
          "bbox": {
            "x0": 108.0,
            "y0": 707.89,
            "x1": 253.97,
            "y1": 717.85
          }
        },
        {
          "text": "• Converts PDF documents to JSON or Markdown format, stable and lightning fast\n• Understands detailed page layout, reading order, locates figures and recovers table struc-",
          "bbox": {
            "x0": 135.4,
            "y0": 671.73,
            "x1": 504.0,
            "y1": 696.55
          }
        },
        {
          "text": "tures",
          "bbox": {
            "x0": 143.87,
            "y0": 660.82,
            "x1": 163.23,
            "y1": 670.78
          }
        },
        {
          "text": "• Extracts metadata from the document, such as title, authors, references and language\n• Optionally applies OCR, e.g. for scanned PDFs\n• Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution)",
          "bbox": {
            "x0": 135.4,
            "y0": 616.23,
            "x1": 504.0,
            "y1": 655.92
          }
        },
        {
          "text": "or interactive mode (compromise on efficiency, low time-to-solution)",
          "bbox": {
            "x0": 143.87,
            "y0": 605.32,
            "x1": 418.87,
            "y1": 615.28
          }
        },
        {
          "text": "• Can leverage different accelerators (GPU, MPS, etc).",
          "bbox": {
            "x0": 135.4,
            "y0": 590.46,
            "x1": 355.41,
            "y1": 600.42
          }
        },
        {
          "text": "2 Getting Started",
          "bbox": {
            "x0": 108.0,
            "y0": 561.74,
            "x1": 205.29,
            "y1": 573.7
          }
        },
        {
          "text": "To use Docling, you can simply install the docling package from PyPI. Documentation and examples\nare available in our GitHub repository at github.com/DS4SD/docling. All required model assets1 are\ndownloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the\nmodel assets in advance.",
          "bbox": {
            "x0": 108.0,
            "y0": 506.28,
            "x1": 504.0,
            "y1": 549.19
          }
        },
        {
          "text": "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary\nstreams, and retrieve the output in either JSON or Markdown format. For convenience, separate\nmethods are offered to convert single documents or batches of documents. A basic usage example\nis illustrated below. Further examples are available in the Doclign code repository.",
          "bbox": {
            "x0": 108.0,
            "y0": 457.16,
            "x1": 504.0,
            "y1": 499.85
          }
        },
        {
          "text": "from docling . docum en t_converter import DocumentConverter",
          "bbox": {
            "x0": 108.75,
            "y0": 441.44,
            "x1": 423.45,
            "y1": 450.41
          }
        },
        {
          "text": "# PDF path or URL",
          "bbox": {
            "x0": 362.66,
            "y0": 421.52,
            "x1": 457.52,
            "y1": 430.48
          }
        },
        {
          "text": "source = \" https :// arxiv . org / pdf /2206.01062 \"\nconverter = DocumentConverter ()\nresult = converter . convert_single ( source )\nprint ( result . re nd er_as_markdown () )",
          "bbox": {
            "x0": 108.78,
            "y0": 391.63,
            "x1": 350.42,
            "y1": 430.48
          }
        },
        {
          "text": "# output : \"## DocLayNet : A Large",
          "bbox": {
            "x0": 311.83,
            "y0": 391.63,
            "x1": 491.34,
            "y1": 400.59
          }
        },
        {
          "text": "Human - Annotated Dataset for Document - Layout Analysis [...]\"",
          "bbox": {
            "x0": 128.6,
            "y0": 381.67,
            "x1": 460.4,
            "y1": 390.63
          }
        },
        {
          "text": "Optionally, you can configure custom pipeline features and runtime options, such as turning on or\noff features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and\ndefining the budget of CPU threads. Advanced usage examples and options are documented in the\nREADME file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a\ncontainer.",
          "bbox": {
            "x0": 108.0,
            "y0": 315.56,
            "x1": 504.0,
            "y1": 369.16
          }
        },
        {
          "text": "3 Processing pipeline",
          "bbox": {
            "x0": 108.0,
            "y0": 286.85,
            "x1": 223.69,
            "y1": 298.81
          }
        },
        {
          "text": "Docling implements a linear pipeline of operations, which execute sequentially on each given docu-\nment (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic\ntext tokens, consisting of string content and its coordinates on the page, and also renders a bitmap\nimage of each page to support downstream operations. Then, the standard model pipeline applies a\nsequence of AI models independently on every page in the document to extract features and content,\nsuch as layout and table structures. Finally, the results from all pages are aggregated and passed\nthrough a post-processing stage, which augments metadata, detects the document language, infers\nreading-order and eventually assembles a typed document object which can be serialized to JSON\nor Markdown.",
          "bbox": {
            "x0": 108.0,
            "y0": 176.84,
            "x1": 504.0,
            "y1": 274.07
          }
        },
        {
          "text": "3.1 PDF backends",
          "bbox": {
            "x0": 108.0,
            "y0": 152.48,
            "x1": 192.03,
            "y1": 162.44
          }
        },
        {
          "text": "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content\nand their geometric coordinates on each page and b) to render the visual representation of each\npage as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling’s\nPDF backend interface. While there are several open-source PDF parsing libraries available for\npython, we faced major obstacles with all of them for different reasons, among which were restrictive",
          "bbox": {
            "x0": 108.0,
            "y0": 88.8,
            "x1": 504.0,
            "y1": 142.4
          }
        },
        {
          "text": "1see huggingface.co/ds4sd/docling-models/",
          "bbox": {
            "x0": 120.65,
            "y0": 70.06,
            "x1": 276.46,
            "y1": 80.5
          }
        },
        {
          "text": "2",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 3,
      "texts": [
        {
          "text": "Figure 1: Sketch of Docling’s default processing pipeline. The inner part of the model pipeline is\neasily customizable and extensible.",
          "bbox": {
            "x0": 108.0,
            "y0": 550.46,
            "x1": 504.0,
            "y1": 571.33
          }
        },
        {
          "text": "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells\nacross far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].",
          "bbox": {
            "x0": 108.0,
            "y0": 504.86,
            "x1": 504.0,
            "y1": 525.73
          }
        },
        {
          "text": "We therefore decided to provide multiple backend choices, and additionally open-source a custom-\nbuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate\npackage named docling-parse and powers the default PDF backend in Docling. As an alternative,\nwe provide a PDF backend relying on pypdfium, which may be a safe backup choice in certain cases,\ne.g. if issues are seen with particular font encodings.",
          "bbox": {
            "x0": 108.0,
            "y0": 444.83,
            "x1": 504.0,
            "y1": 498.43
          }
        },
        {
          "text": "3.2 AI models",
          "bbox": {
            "x0": 108.0,
            "y0": 417.47,
            "x1": 173.86,
            "y1": 427.43
          }
        },
        {
          "text": "As part of Docling, we initially release two highly capable AI models to the open-source community,\nwhich have been developed and published recently by our team. The first model is a layout analysis\nmodel, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9],\na state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on\nhuggingface) and a separate package for the inference code as docling-ibm-models. Both models\nare also powering the open-access deepsearch-experience, our cloud-native service for knowledge\nexploration tasks.",
          "bbox": {
            "x0": 108.0,
            "y0": 330.78,
            "x1": 504.0,
            "y1": 406.2
          }
        },
        {
          "text": "Layout Analysis Model",
          "bbox": {
            "x0": 108.0,
            "y0": 304.76,
            "x1": 206.28,
            "y1": 314.72
          }
        },
        {
          "text": "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of\nvarious elements on the image of a given page. Its architecture is derived from RT-DETR [16] and\nre-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis,\namong other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].",
          "bbox": {
            "x0": 108.0,
            "y0": 252.15,
            "x1": 504.0,
            "y1": 294.83
          }
        },
        {
          "text": "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single\nCPU with sub-second latency. All predicted bounding-box proposals for document elements are\npost-processed to remove overlapping proposals based on confidence and size, and then intersected\nwith the text tokens in the PDF to group them into meaningful and complete units such as paragraphs,\nsection titles, list items, captions, figures or tables.",
          "bbox": {
            "x0": 108.0,
            "y0": 192.12,
            "x1": 504.0,
            "y1": 245.72
          }
        },
        {
          "text": "Table Structure Recognition",
          "bbox": {
            "x0": 108.0,
            "y0": 166.1,
            "x1": 228.11,
            "y1": 176.06
          }
        },
        {
          "text": "The TableFormer model [12], first published in 2022 and since refined with a custom structure token\nlanguage [9], is a vision-transformer model for table structure recovery. It can predict the logical\nrow and column structure of a given table based on an input image, and determine which table\ncells belong to column headers, row headers or the table body. Compared to earlier approaches,\nTableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells,\nrows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with\ninconsistent indentation or alignment and other complexities. For inference, our implementation\nrelies on PyTorch [2].",
          "bbox": {
            "x0": 108.0,
            "y0": 69.85,
            "x1": 504.0,
            "y1": 156.17
          }
        },
        {
          "text": "3",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 4,
      "texts": [
        {
          "text": "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model,\nby providing an image-crop of the table and the included text cells. TableFormer structure predic-\ntions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text\nin the table image. Typical tables require between 2 and 6 seconds to be processed on a standard\nCPU, strongly depending on the amount of included table cells.",
          "bbox": {
            "x0": 108.0,
            "y0": 664.25,
            "x1": 504.0,
            "y1": 717.85
          }
        },
        {
          "text": "OCR",
          "bbox": {
            "x0": 108.0,
            "y0": 641.68,
            "x1": 130.14,
            "y1": 651.64
          }
        },
        {
          "text": "Docling provides optional support for OCR, for example to cover scanned PDFs or content in\nbitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular third-\nparty OCR library with support for many languages. Docling, by default, feeds a high-resolution\npage image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality.\nWhile EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on\nCPU (upwards of 30 seconds per page).",
          "bbox": {
            "x0": 108.0,
            "y0": 568.43,
            "x1": 504.0,
            "y1": 632.94
          }
        },
        {
          "text": "We are actively seeking collaboration from the open-source community to extend Docling with\nadditional OCR backends and speed improvements.",
          "bbox": {
            "x0": 108.0,
            "y0": 541.14,
            "x1": 504.0,
            "y1": 562.01
          }
        },
        {
          "text": "3.3 Assembly",
          "bbox": {
            "x0": 108.0,
            "y0": 517.22,
            "x1": 171.37,
            "y1": 527.18
          }
        },
        {
          "text": "In the final pipeline stage, Docling assembles all prediction results produced on each page into a\nwell-defined datatype that encapsulates a converted document, as defined in the auxiliary package\ndocling-core. The generated document object is passed through a post-processing model which\nleverages several algorithms to augment features, such as detection of the document language, cor-\nrecting the reading order, matching figures with captions and labelling metadata such as title, authors\nand references. The final output can then be serialized to JSON or transformed into a Markdown\nrepresentation at the users request.",
          "bbox": {
            "x0": 108.0,
            "y0": 431.72,
            "x1": 504.0,
            "y1": 507.14
          }
        },
        {
          "text": "3.4 Extensibility",
          "bbox": {
            "x0": 108.0,
            "y0": 407.8,
            "x1": 184.11,
            "y1": 417.77
          }
        },
        {
          "text": "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline.\nA model pipeline constitutes the central part in the processing, following initial document parsing\nand preceding output assembly, and can be fully customized by sub-classing from an abstract base-\nclass (BaseModelPipeline) or cloning the default model pipeline. This effectively allows to fully\ncustomize the chain of models, add or replace models, and introduce additional pipeline config-\nuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can\nbe provided as an argument to the main document conversion methods. We invite everyone in the\ncommunity to propose additional or alternative models and improvements.",
          "bbox": {
            "x0": 108.0,
            "y0": 311.39,
            "x1": 504.0,
            "y1": 397.72
          }
        },
        {
          "text": "Implementations of model classes must satisfy the python Callable interface. The __call__ method\nmust accept an iterator over page objects, and produce another iterator over the page objects which\nwere augmented with the additional features predicted by the model, by extending the provided\nPagePredictions data model accordingly.",
          "bbox": {
            "x0": 108.0,
            "y0": 262.28,
            "x1": 504.0,
            "y1": 304.97
          }
        },
        {
          "text": "4 Performance",
          "bbox": {
            "x0": 108.0,
            "y0": 234.01,
            "x1": 191.78,
            "y1": 245.97
          }
        },
        {
          "text": "In this section, we establish some reference numbers for the processing speed of Docling and the\nresource budget it requires. All tests in this section are run with default options on our standard test\nset distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks,\nwith a total of 225 pages. Measurements were taken using both available PDF backends on two\ndifferent hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu\n20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through\nsetting OMP NUM THREADS environment variable) once to 4 (Docling default) and once to 16\n(equal to full core count on the test hardware). All results are shown in Table 1.",
          "bbox": {
            "x0": 108.0,
            "y0": 135.35,
            "x1": 504.0,
            "y1": 221.68
          }
        },
        {
          "text": "If you need to run Docling in very low-resource environments, please consider configuring the\npypdfium backend. While it is faster and more memory efficient than the default docling-parse\nbackend, it will come at the expense of worse quality results, especially in table structure recovery.",
          "bbox": {
            "x0": 108.0,
            "y0": 97.15,
            "x1": 504.0,
            "y1": 128.93
          }
        },
        {
          "text": "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely\nuntested, but may work implicitly when CUDA is available and discovered by the onnxruntime and",
          "bbox": {
            "x0": 108.0,
            "y0": 69.85,
            "x1": 504.0,
            "y1": 90.72
          }
        },
        {
          "text": "4",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 5,
      "texts": [
        {
          "text": "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future\nversion of this report.",
          "bbox": {
            "x0": 108.0,
            "y0": 696.98,
            "x1": 504.0,
            "y1": 717.85
          }
        },
        {
          "text": "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our\ntest dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution\n(TTS), computed throughput in pages per second, and the peak memory used (resident set size) for\nboth the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.",
          "bbox": {
            "x0": 108.0,
            "y0": 643.98,
            "x1": 504.0,
            "y1": 686.67
          }
        },
        {
          "text": "CPU",
          "bbox": {
            "x0": 134.52,
            "y0": 617.18,
            "x1": 153.9,
            "y1": 627.14
          }
        },
        {
          "text": "Thread\nbudget",
          "bbox": {
            "x0": 207.91,
            "y0": 611.73,
            "x1": 236.12,
            "y1": 632.6
          }
        },
        {
          "text": "Apple M3 Max\n(16 cores)",
          "bbox": {
            "x0": 134.52,
            "y0": 580.1,
            "x1": 195.95,
            "y1": 600.97
          }
        },
        {
          "text": "Intel(R) Xeon\nE5-2690\n(16 cores)",
          "bbox": {
            "x0": 134.52,
            "y0": 542.37,
            "x1": 190.14,
            "y1": 574.15
          }
        },
        {
          "text": "4\n16",
          "bbox": {
            "x0": 207.91,
            "y0": 580.1,
            "x1": 217.87,
            "y1": 600.97
          }
        },
        {
          "text": "4\n16",
          "bbox": {
            "x0": 207.91,
            "y0": 553.28,
            "x1": 217.87,
            "y1": 574.15
          }
        },
        {
          "text": "native backend",
          "bbox": {
            "x0": 272.67,
            "y0": 622.64,
            "x1": 332.21,
            "y1": 632.6
          }
        },
        {
          "text": "pypdfium backend",
          "bbox": {
            "x0": 383.28,
            "y0": 622.64,
            "x1": 456.97,
            "y1": 632.6
          }
        },
        {
          "text": "TTS",
          "bbox": {
            "x0": 249.88,
            "y0": 606.92,
            "x1": 267.59,
            "y1": 616.89
          }
        },
        {
          "text": "Pages/s",
          "bbox": {
            "x0": 281.34,
            "y0": 606.92,
            "x1": 311.08,
            "y1": 616.89
          }
        },
        {
          "text": "Mem",
          "bbox": {
            "x0": 329.4,
            "y0": 606.92,
            "x1": 350.43,
            "y1": 616.89
          }
        },
        {
          "text": "TTS",
          "bbox": {
            "x0": 370.55,
            "y0": 606.92,
            "x1": 388.27,
            "y1": 616.89
          }
        },
        {
          "text": "Pages/s",
          "bbox": {
            "x0": 402.01,
            "y0": 606.92,
            "x1": 431.75,
            "y1": 616.89
          }
        },
        {
          "text": "Mem",
          "bbox": {
            "x0": 450.07,
            "y0": 606.92,
            "x1": 471.11,
            "y1": 616.89
          }
        },
        {
          "text": "177 s\n167 s",
          "bbox": {
            "x0": 248.08,
            "y0": 580.1,
            "x1": 269.39,
            "y1": 600.97
          }
        },
        {
          "text": "375 s\n244 s",
          "bbox": {
            "x0": 248.08,
            "y0": 553.28,
            "x1": 269.39,
            "y1": 574.15
          }
        },
        {
          "text": "1.27\n1.34",
          "bbox": {
            "x0": 287.5,
            "y0": 580.1,
            "x1": 304.93,
            "y1": 600.97
          }
        },
        {
          "text": "0.60\n0.92",
          "bbox": {
            "x0": 287.5,
            "y0": 553.28,
            "x1": 304.93,
            "y1": 574.15
          }
        },
        {
          "text": "6.20 GB",
          "bbox": {
            "x0": 323.04,
            "y0": 585.56,
            "x1": 356.8,
            "y1": 595.52
          }
        },
        {
          "text": "6.16 GB",
          "bbox": {
            "x0": 323.04,
            "y0": 558.74,
            "x1": 356.8,
            "y1": 568.7
          }
        },
        {
          "text": "103 s\n92 s",
          "bbox": {
            "x0": 368.75,
            "y0": 580.1,
            "x1": 390.06,
            "y1": 600.97
          }
        },
        {
          "text": "239 s\n143 s",
          "bbox": {
            "x0": 368.75,
            "y0": 553.28,
            "x1": 390.06,
            "y1": 574.15
          }
        },
        {
          "text": "2.18\n2.45",
          "bbox": {
            "x0": 408.17,
            "y0": 580.1,
            "x1": 425.61,
            "y1": 600.97
          }
        },
        {
          "text": "0.94\n1.57",
          "bbox": {
            "x0": 408.17,
            "y0": 553.28,
            "x1": 425.61,
            "y1": 574.15
          }
        },
        {
          "text": "2.56 GB",
          "bbox": {
            "x0": 443.71,
            "y0": 585.56,
            "x1": 477.48,
            "y1": 595.52
          }
        },
        {
          "text": "2.42 GB",
          "bbox": {
            "x0": 443.71,
            "y0": 558.74,
            "x1": 477.48,
            "y1": 568.7
          }
        },
        {
          "text": "5 Applications",
          "bbox": {
            "x0": 108.0,
            "y0": 517.88,
            "x1": 190.07,
            "y1": 529.84
          }
        },
        {
          "text": "Thanks to the high-quality, richly structured document conversion achieved by Docling, its out-\nput qualifies for numerous downstream applications. For example, Docling can provide a base\nfor detailed enterprise document search, passage retrieval or classification use-cases, or support\nknowledge extraction pipelines, allowing specific treatment of different structures in the document,\nsuch as tables, figures, section structure or references. For popular generative AI application pat-\nterns, such as retrieval-augmented generation (RAG), we provide quackling, an open-source package\nwhich capitalizes on Docling’s feature-rich document output to enable document-native optimized\nvector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIn-\ndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build\ndocument-derived datasets. With its powerful table structure recognition, it provides significant ben-\nefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open\nIBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal\ntraining datasets.",
          "bbox": {
            "x0": 108.0,
            "y0": 364.49,
            "x1": 504.0,
            "y1": 505.36
          }
        },
        {
          "text": "6 Future work and contributions",
          "bbox": {
            "x0": 108.0,
            "y0": 336.04,
            "x1": 283.78,
            "y1": 348.0
          }
        },
        {
          "text": "Docling is designed to allow easy extension of the model library and pipelines. In the future, we\nplan to extend Docling with several more models, such as a figure-classifier model, an equation-\nrecognition model, a code-recognition model and more. This will help improve the quality of con-\nversion for specific types of content, as well as augment extracted document metadata with ad-\nditional information. Further investment into testing and optimizing GPU acceleration as well as\nimproving the Docling-native PDF backend are on our roadmap, too.",
          "bbox": {
            "x0": 108.0,
            "y0": 259.02,
            "x1": 504.0,
            "y1": 323.52
          }
        },
        {
          "text": "We encourage everyone to propose or implement additional features and models, and will\ngladly take your inputs and contributions under review. The codebase of Docling is open for use\nand contribution, under the MIT license agreement and in alignment with our contributing guidelines\nincluded in the Docling repository. If you use Docling in your projects, please consider citing this\ntechnical report.",
          "bbox": {
            "x0": 108.0,
            "y0": 198.99,
            "x1": 504.0,
            "y1": 252.66
          }
        },
        {
          "text": "References",
          "bbox": {
            "x0": 108.0,
            "y0": 170.54,
            "x1": 163.54,
            "y1": 182.5
          }
        },
        {
          "text": "[1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/",
          "bbox": {
            "x0": 112.98,
            "y0": 153.48,
            "x1": 504.0,
            "y1": 163.5
          }
        },
        {
          "text": "JaidedAI/EasyOCR, 2024. Version: 1.7.0.",
          "bbox": {
            "x0": 129.58,
            "y0": 142.57,
            "x1": 302.96,
            "y1": 152.59
          }
        },
        {
          "text": "[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard,\nE. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison,\nW. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. La-\nzos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso,\nM. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang,\nX. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster",
          "bbox": {
            "x0": 112.98,
            "y0": 69.85,
            "x1": 504.0,
            "y1": 134.36
          }
        },
        {
          "text": "5",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 6,
      "texts": [
        {
          "text": "machine learning through dynamic python bytecode transformation and graph compilation.\nIn Proceedings of the 29th ACM International Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems, Volume 2 (ASPLOS ’24). ACM, 4 2024. doi:\n10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf.",
          "bbox": {
            "x0": 129.58,
            "y0": 675.1,
            "x1": 504.0,
            "y1": 717.85
          }
        },
        {
          "text": "[3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion\nas a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International\nConference on Cloud Computing (CLOUD), pages 363–373. IEEE, 2022.",
          "bbox": {
            "x0": 112.98,
            "y0": 634.46,
            "x1": 504.0,
            "y1": 666.24
          }
        },
        {
          "text": "[4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https:",
          "bbox": {
            "x0": 112.98,
            "y0": 615.52,
            "x1": 504.0,
            "y1": 625.54
          }
        },
        {
          "text": "//github.com/qpdf/qpdf.",
          "bbox": {
            "x0": 129.58,
            "y0": 604.61,
            "x1": 247.14,
            "y1": 614.63
          }
        },
        {
          "text": "[5] O. R. developers. Onnx runtime. https://onnxruntime.ai/, 2024. Version: 1.18.1.",
          "bbox": {
            "x0": 112.98,
            "y0": 585.73,
            "x1": 478.89,
            "y1": 595.76
          }
        },
        {
          "text": "[6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured\nURL https://github.com/IBM/",
          "bbox": {
            "x0": 112.98,
            "y0": 555.95,
            "x1": 504.0,
            "y1": 576.88
          }
        },
        {
          "text": "data preparation for LLM app developers, 2024.\ndata-prep-kit.",
          "bbox": {
            "x0": 129.58,
            "y0": 545.04,
            "x1": 343.03,
            "y1": 565.97
          }
        },
        {
          "text": "[7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF.",
          "bbox": {
            "x0": 112.98,
            "y0": 526.16,
            "x1": 447.42,
            "y1": 536.18
          }
        },
        {
          "text": "[8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.",
          "bbox": {
            "x0": 112.98,
            "y0": 507.28,
            "x1": 483.9,
            "y1": 517.3
          }
        },
        {
          "text": "[9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization\nfor Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th\nInternational Conference, San Jos´e, CA, USA, August 21–26, 2023, Proceedings, Part II, pages\n37–50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10.\n1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3.",
          "bbox": {
            "x0": 112.98,
            "y0": 444.76,
            "x1": 504.0,
            "y1": 498.42
          }
        },
        {
          "text": "[10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. State-\nments: Universal information extraction from tables with large language models for ESG\nIn D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christi-\nKPIs.\naen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the\n1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024),\npages 193–214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics.\nURL https://aclanthology.org/2024.climatenlp-1.15.",
          "bbox": {
            "x0": 108.0,
            "y0": 360.43,
            "x1": 504.0,
            "y1": 435.91
          }
        },
        {
          "text": "[11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of\nchemical structures in patent documents. Nature Communications, 15(1):6532, August 2024.\nISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/\ns41467-024-50779-y.",
          "bbox": {
            "x0": 108.0,
            "y0": 308.82,
            "x1": 504.0,
            "y1": 351.57
          }
        },
        {
          "text": "[12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and",
          "bbox": {
            "x0": 108.0,
            "y0": 279.09,
            "x1": 504.0,
            "y1": 299.97
          }
        },
        {
          "text": "with transformers.\nPattern Recognition, pages 4614–4623, 2022.",
          "bbox": {
            "x0": 129.58,
            "y0": 268.18,
            "x1": 311.6,
            "y1": 289.06
          }
        },
        {
          "text": "[13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large human-",
          "bbox": {
            "x0": 108.0,
            "y0": 249.31,
            "x1": 504.0,
            "y1": 259.27
          }
        },
        {
          "text": "annotated dataset for document-layout segmentation. pages 3743–3751, 2022.",
          "bbox": {
            "x0": 129.58,
            "y0": 238.4,
            "x1": 441.54,
            "y1": 248.36
          }
        },
        {
          "text": "[14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/",
          "bbox": {
            "x0": 108.0,
            "y0": 219.46,
            "x1": 504.0,
            "y1": 229.48
          }
        },
        {
          "text": "py-pdf/pypdf.",
          "bbox": {
            "x0": 129.58,
            "y0": 208.55,
            "x1": 195.33,
            "y1": 218.57
          }
        },
        {
          "text": "[15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/",
          "bbox": {
            "x0": 108.0,
            "y0": 189.67,
            "x1": 504.0,
            "y1": 199.69
          }
        },
        {
          "text": "pypdfium2-team/pypdfium2.",
          "bbox": {
            "x0": 129.58,
            "y0": 178.76,
            "x1": 258.1,
            "y1": 188.78
          }
        },
        {
          "text": "[16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on",
          "bbox": {
            "x0": 108.0,
            "y0": 159.94,
            "x1": 504.0,
            "y1": 169.9
          }
        },
        {
          "text": "real-time object detection, 2023.",
          "bbox": {
            "x0": 129.58,
            "y0": 149.03,
            "x1": 258.79,
            "y1": 158.99
          }
        },
        {
          "text": "6",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 7,
      "texts": [
        {
          "text": "Appendix",
          "bbox": {
            "x0": 108.0,
            "y0": 707.54,
            "x1": 157.53,
            "y1": 719.49
          }
        },
        {
          "text": "In this section, we illustrate a few examples of Docling’s output in Markdown and JSON.",
          "bbox": {
            "x0": 108.0,
            "y0": 684.77,
            "x1": 463.75,
            "y1": 694.73
          }
        },
        {
          "text": "Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered\nMarkdown. If recognized, metadata such as authors are appearing first under the title. Text content\ninside figures is currently dropped, the caption is retained and linked to the figure in the JSON\nrepresentation (not shown).",
          "bbox": {
            "x0": 108.0,
            "y0": 377.69,
            "x1": 504.0,
            "y1": 420.38
          }
        },
        {
          "text": "7",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 8,
      "texts": [
        {
          "text": "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing\nfirst under the title. Elements recognized as page headers or footers are suppressed in Markdown to\ndeliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph\nin ”5. Experiments” wrapping over the column end is broken up in two and interrupted by the table.",
          "bbox": {
            "x0": 108.0,
            "y0": 225.06,
            "x1": 504.0,
            "y1": 267.75
          }
        },
        {
          "text": "8",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    },
    {
      "page_number": 9,
      "texts": [
        {
          "text": "Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B)\nand in JSON representation (C). Spanning table cells, such as the multi-column header ”triple inter-\nannotator mAP@0.5-0.95 (%)”, is repeated for each column in the Markdown representation (B),\nwhich guarantees that every data point can be traced back to row and column headings only by its\ngrid coordinates in the table. In the JSON representation, the span information is reflected in the\nfields of each table cell (C).",
          "bbox": {
            "x0": 108.0,
            "y0": 213.96,
            "x1": 504.0,
            "y1": 278.46
          }
        },
        {
          "text": "9",
          "bbox": {
            "x0": 303.51,
            "y0": 39.96,
            "x1": 308.49,
            "y1": 49.92
          }
        }
      ]
    }
  ]
}